{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-node multi-GPU example on Azure using dask-cloudprovider\n",
    "\n",
    "[Dask Cloud Provider](https://cloudprovider.dask.org/en/latest/) is a native cloud intergration for dask. It helps manage Dask clusters on different cloud platforms. In this notebook, we will look at how we can use the package to set-up a Azure cluster and run a multi-node, multi-GPU example with [RAPIDS](https://rapids.ai/). RAPIDS provides a suite of libraries to accelerate data science pipelines on the GPU entirely. This can be scaled to multiple nodes using Dask as we will see through this notebook. \n",
    "\n",
    "For the purposes of this demo, we will use the NYC Taxi Dataset and run on a part of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the notebook, run the following commands in the terminal to setup Azure CLI\n",
    "```\n",
    "pip install azure-cli\n",
    "az login\n",
    "```\n",
    "\n",
    "The list of packages needed for this notebook is listed in the cell below - uncomment and run the cell to set it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"dask-cloudprovider[azure]\"\n",
    "# !pip install \"dask-cloudprovider[azure]\" --upgrade\n",
    "# !pip install --upgrade azure-mgmt-network azure-mgmt-compute\n",
    "# !pip install gcsfs\n",
    "# !pip install dask_xgboost\n",
    "# !pip install azureml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "import dask\n",
    "import cudf\n",
    "import dask_cudf\n",
    "\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from cuml.metrics import mean_squared_error\n",
    "\n",
    "from cuml.dask.ensemble import RandomForestRegressor\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba, xgboost, socket\n",
    "import dask, dask_cudf\n",
    "from dask.distributed import Client, wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure cluster set up\n",
    "\n",
    "Let us now setup the [Azure cluster](https://cloudprovider.dask.org/en/latest/azure.html) using `AzureVMCluster` from Dask Cloud Provider. To do this, you;ll first need to set up a Resource Group, a Virtual Network and a Security Group on Azure. [Learn more about how you can set this up](https://cloudprovider.dask.org/en/latest/azure.html#resource-groups). Note that you can also set it up using the Azure portal directly.\n",
    "\n",
    "Once you have set it up, you can now plug in the names of the entities you have created in the cell below. Finally note that we use the RAPIDS docker image to build the VM and use the `dask_cuda.CUDAWorker` to run within the VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"\"\n",
    "resource_group = \"\"\n",
    "vnet = \"\"\n",
    "security_group = \"\"\n",
    "\n",
    "vm_size = \"Standard_NC12s_v3\"\n",
    "docker_image = \"rapidsai/rapidsai:cuda10.2-runtime-ubuntu18.04-py3.8\"\n",
    "worker_class = \"dask_cuda.CUDAWorker\"\n",
    " \n",
    "n_workers = 1\n",
    "env_vars = {\"EXTRA_PIP_PACKAGES\": \"gcsfs\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating scheduler instance\n",
      "Assigned public IP\n",
      "Network interface ready\n",
      "Creating VM\n",
      "Created VM dask-b505b93e-scheduler\n",
      "Waiting for scheduler to run\n",
      "Scheduler is running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/contextlib.py:120: UserWarning: Creating your cluster is taking a surprisingly long time. This is likely due to pending resources. Hang tight! \n",
      "  next(self.gen)\n"
     ]
    }
   ],
   "source": [
    "from distributed import Client\n",
    "from dask_cloudprovider.azure import AzureVMCluster\n",
    "\n",
    "cluster = AzureVMCluster(\n",
    "    location=location,\n",
    "    resource_group=resource_group,\n",
    "    vnet=vnet,\n",
    "    security_group=security_group,\n",
    "    vm_size=vm_size,\n",
    "    docker_image=docker_image,\n",
    "    worker_class=worker_class,\n",
    "    env_vars=env_vars,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data locally to see what we're dealing with. We will make use of the data from 2014 for the purposes of the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>rate_code</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>surcharge</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CMT</td>\n",
       "      <td>2014-01-09 20:45:25</td>\n",
       "      <td>2014-01-09 20:52:31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-73.994770</td>\n",
       "      <td>40.736828</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.982227</td>\n",
       "      <td>40.731790</td>\n",
       "      <td>CRD</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CMT</td>\n",
       "      <td>2014-01-09 20:46:12</td>\n",
       "      <td>2014-01-09 20:55:12</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-73.982392</td>\n",
       "      <td>40.773382</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.960449</td>\n",
       "      <td>40.763995</td>\n",
       "      <td>CRD</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CMT</td>\n",
       "      <td>2014-01-09 20:44:47</td>\n",
       "      <td>2014-01-09 20:59:46</td>\n",
       "      <td>2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>-73.988570</td>\n",
       "      <td>40.739406</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.986626</td>\n",
       "      <td>40.765217</td>\n",
       "      <td>CRD</td>\n",
       "      <td>11.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CMT</td>\n",
       "      <td>2014-01-09 20:44:57</td>\n",
       "      <td>2014-01-09 20:51:40</td>\n",
       "      <td>1</td>\n",
       "      <td>1.7</td>\n",
       "      <td>-73.960213</td>\n",
       "      <td>40.770464</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.979863</td>\n",
       "      <td>40.777050</td>\n",
       "      <td>CRD</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CMT</td>\n",
       "      <td>2014-01-09 20:47:09</td>\n",
       "      <td>2014-01-09 20:53:32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-73.995371</td>\n",
       "      <td>40.717248</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.984367</td>\n",
       "      <td>40.720524</td>\n",
       "      <td>CRD</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vendor_id      pickup_datetime     dropoff_datetime   passenger_count  \\\n",
       "0       CMT  2014-01-09 20:45:25  2014-01-09 20:52:31                 1   \n",
       "1       CMT  2014-01-09 20:46:12  2014-01-09 20:55:12                 1   \n",
       "2       CMT  2014-01-09 20:44:47  2014-01-09 20:59:46                 2   \n",
       "3       CMT  2014-01-09 20:44:57  2014-01-09 20:51:40                 1   \n",
       "4       CMT  2014-01-09 20:47:09  2014-01-09 20:53:32                 1   \n",
       "\n",
       "    trip_distance   pickup_longitude   pickup_latitude   rate_code  \\\n",
       "0             0.7         -73.994770         40.736828           1   \n",
       "1             1.4         -73.982392         40.773382           1   \n",
       "2             2.3         -73.988570         40.739406           1   \n",
       "3             1.7         -73.960213         40.770464           1   \n",
       "4             0.9         -73.995371         40.717248           1   \n",
       "\n",
       "   store_and_fwd_flag   dropoff_longitude   dropoff_latitude  payment_type  \\\n",
       "0                   N          -73.982227          40.731790           CRD   \n",
       "1                   N          -73.960449          40.763995           CRD   \n",
       "2                   N          -73.986626          40.765217           CRD   \n",
       "3                   N          -73.979863          40.777050           CRD   \n",
       "4                   N          -73.984367          40.720524           CRD   \n",
       "\n",
       "    fare_amount   surcharge   mta_tax   tip_amount   tolls_amount  \\\n",
       "0           6.5         0.5       0.5         1.40            0.0   \n",
       "1           8.5         0.5       0.5         1.90            0.0   \n",
       "2          11.5         0.5       0.5         1.50            0.0   \n",
       "3           7.5         0.5       0.5         1.70            0.0   \n",
       "4           6.0         0.5       0.5         1.75            0.0   \n",
       "\n",
       "    total_amount  \n",
       "0           8.90  \n",
       "1          11.40  \n",
       "2          14.00  \n",
       "3          10.20  \n",
       "4           8.75  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = 'gcs://anaconda-public-data/nyc-taxi/csv/'\n",
    "tmp_df = dask_cudf.read_csv(base_path+'2014/yellow_tripdata_2014*.csv', n_rows=1000)\n",
    "\n",
    "tmp_df.head().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleanup\n",
    "\n",
    "The data needs to be cleaned up before it can be used in a meaningful way. We first perform a renaming of some columns to a cleaner name (for instance, some of the years have `tpep_ropoff_datetime` instead of `dropfoff_datetime`). We also define the datatypes each of the columns need to be read as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names that need to be re-mapped\n",
    "remap = {}\n",
    "remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "#create a list of columns & dtypes the df must have\n",
    "must_haves = {\n",
    " 'pickup_datetime': 'datetime64[ms]',\n",
    " 'dropoff_datetime': 'datetime64[ms]',\n",
    " 'passenger_count': 'int32',\n",
    " 'trip_distance': 'float32',\n",
    " 'pickup_longitude': 'float32',\n",
    " 'pickup_latitude': 'float32',\n",
    " 'rate_code': 'int32',\n",
    " 'dropoff_longitude': 'float32',\n",
    " 'dropoff_latitude': 'float32',\n",
    " 'fare_amount': 'float32'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df_part, remap, must_haves):\n",
    "    \"\"\"\n",
    "    This function performs the various clean up tasks for the data\n",
    "    and returns the cleaned dataframe.\n",
    "    \"\"\"\n",
    "    tmp = {col:col.strip().lower() for col in list(df_part.columns)}\n",
    "    df_part = df_part.rename(columns=tmp)\n",
    "    # rename using the supplied mapping\n",
    "    df_part = df_part.rename(columns=remap)\n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part = df_part.drop(col, axis=1)\n",
    "            continue\n",
    "\n",
    "        # fixes datetime error found by Ty Mckercher and fixed by Paul Mahler\n",
    "        if df_part[col].dtype == 'object' and col in ['pickup_datetime', 'dropoff_datetime']:\n",
    "            df_part[col] = df_part[col].astype('datetime64[ms]')\n",
    "            continue\n",
    "\n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].str.fillna('-1')\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('float32')\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Interesting Features\n",
    "\n",
    "We'll add new features by making use of \"uder defined functions\" on the dataframe. We'll make use of [apply_rows](https://docs.rapids.ai/api/cudf/stable/api.html#cudf.core.dataframe.DataFrame.apply_rows), which is similar to Pandas' apply funciton. `apply_rows` operation is [JIT compiled by numba](https://numba.pydata.org/numba-doc/dev/cuda/kernels.html) into GPU kernels. \n",
    "\n",
    "The kernels we define are - \n",
    "1. Haversine distance: This is used for calculating the total trip distance.\n",
    "\n",
    "2. Day of the week: This can be useful information for determining the fare cost.\n",
    "\n",
    "`add_features` function combined the two to produce a new dataframe that has the added features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "\n",
    "def haversine_distance_kernel(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude, h_distance):\n",
    "    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude)):\n",
    "        x_1 = pi/180 * x_1\n",
    "        y_1 = pi/180 * y_1\n",
    "        x_2 = pi/180 * x_2\n",
    "        y_2 = pi/180 * y_2\n",
    "        \n",
    "        dlon = y_2 - y_1\n",
    "        dlat = x_2 - x_1\n",
    "        a = sin(dlat/2)**2 + cos(x_1) * cos(x_2) * sin(dlon/2)**2\n",
    "        \n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        r = 6371 # Radius of earth in kilometers\n",
    "        \n",
    "        h_distance[i] = c * r\n",
    "\n",
    "def day_of_the_week_kernel(day, month, year, day_of_week):\n",
    "    for i, (d_1, m_1, y_1) in enumerate(zip(day, month, year)):\n",
    "        if month[i] <3:\n",
    "            shift = month[i]\n",
    "        else:\n",
    "            shift = 0\n",
    "        Y = year[i] - (month[i] < 3)\n",
    "        y = Y - 2000\n",
    "        c = 20\n",
    "        d = day[i]\n",
    "        m = month[i] + shift + 1\n",
    "        day_of_week[i] = (d + math.floor(m*2.6) + y + (y//4) + (c//4) -2*c)%7\n",
    "        \n",
    "def add_features(df):\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day'] = df['pickup_datetime'].dt.day\n",
    "    df['diff'] = df['dropoff_datetime'].astype('int32') - df['pickup_datetime'].astype('int32')\n",
    "    \n",
    "    df['pickup_latitude_r'] = df['pickup_latitude']//.01*.01\n",
    "    df['pickup_longitude_r'] = df['pickup_longitude']//.01*.01\n",
    "    df['dropoff_latitude_r'] = df['dropoff_latitude']//.01*.01\n",
    "    df['dropoff_longitude_r'] = df['dropoff_longitude']//.01*.01\n",
    "    \n",
    "    df = df.drop('pickup_datetime', axis=1)\n",
    "    df = df.drop('dropoff_datetime', axis =1)\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(haversine_distance_kernel,\n",
    "                   incols=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'],\n",
    "                   outcols=dict(h_distance=np.float32),\n",
    "                   kwargs=dict())\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(day_of_the_week_kernel,\n",
    "                      incols=['day', 'month', 'year'],\n",
    "                      outcols=dict(day_of_week=np.float32),\n",
    "                      kwargs=dict())\n",
    "    \n",
    "    \n",
    "    df['is_weekend'] = (df['day_of_week']<2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RF model\n",
    "\n",
    "We are now ready to fit a Random Forest on the data to predict the fare for the trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_rf_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 16,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below creates a client with the cluster we defined earlier in the notebook. Note that we have `cluster.scale`. This is the step where the workers are allocated.\n",
    "\n",
    "Once workers become available, we can now run the rest of our workflow - reading and cleaning the data, splitting into training and validation sets, fitting a RF model and predicting on the validation set. We print out the MSE metric for this problem. Note that for better performance we should perform HPO ideally. \n",
    "\n",
    "\n",
    "Refer to the notebooks in the repository for how to perform automated HPO [using RayTune](https://github.com/rapidsai/cloud-ml-examples/blob/main/ray/notebooks/Ray_RAPIDS_HPO.ipynb) and [using Optuna](https://github.com/rapidsai/cloud-ml-examples/blob/main/optuna/notebooks/optuna_rapids.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  3.3357165\n"
     ]
    }
   ],
   "source": [
    "with Client(cluster) as client:\n",
    "    import dask_cudf\n",
    "    cluster.scale(2)\n",
    "    client.wait_for_workers(2)\n",
    "    from cuml.dask.ensemble import RandomForestRegressor\n",
    "\n",
    "    base_path = 'gcs://anaconda-public-data/nyc-taxi/csv/'\n",
    "    df_2014 = dask_cudf.read_csv(base_path+'2014/yellow_tripdata_2014*.csv', n_rows=100000)\n",
    "\n",
    "    df_2014 = clean(df_2014, remap, must_haves)\n",
    "\n",
    "    taxi_df = df_2014.map_partitions(add_features)\n",
    "\n",
    "    taxi_df = taxi_df.dropna()\n",
    "    taxi_df = taxi_df.astype(\"float32\")\n",
    "    X, y = taxi_df.drop([\"fare_amount\"], axis=1), taxi_df[\"fare_amount\"].astype('float32')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
    "\n",
    "    workers = client.has_what().keys()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = dask_utils.persist_across_workers(client,\n",
    "                                                           [X_train, X_test, y_train, y_test],\n",
    "                                                           workers=workers)\n",
    "    cu_dask_rf = RandomForestRegressor(**cu_rf_params, ignore_empty_partitions=True)\n",
    "    cu_dask_rf = cu_dask_rf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = cu_dask_rf.predict(X_test)\n",
    "    \n",
    "    _y_pred, _y_test = y_pred.compute().to_array(), y_test.compute().to_array()\n",
    "    \n",
    "    score = mean_squared_error(_y_pred, _y_test)\n",
    "    print(\"RMSE: \", np.sqrt(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminated VM dask-b505b93e-worker-11afc90d\n",
      "Removed disks for VM dask-b505b93e-worker-11afc90d\n",
      "Deleted network interface\n",
      "Terminated VM dask-b505b93e-worker-dbb420e7\n",
      "Removed disks for VM dask-b505b93e-worker-dbb420e7\n",
      "Deleted network interface\n",
      "Terminated VM dask-b505b93e-scheduler\n",
      "Removed disks for VM dask-b505b93e-scheduler\n",
      "Deleted network interface\n",
      "Unassigned public IP\n"
     ]
    }
   ],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
