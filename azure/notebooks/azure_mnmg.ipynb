{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"dask-cloudprovider[azure]\"\n",
    "# !pip install \"dask-cloudprovider[azure]\" --upgrade\n",
    "# !pip install --upgrade azure-mgmt-network azure-mgmt-compute\n",
    "# !pip install gcsfs\n",
    "# !pip install dask_xgboost\n",
    "# !pip install azureml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "import dask\n",
    "import cudf\n",
    "import dask_cudf\n",
    "\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from cuml.metrics import mean_squared_error\n",
    "\n",
    "from cuml.dask.ensemble import RandomForestRegressor\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba, xgboost, socket\n",
    "import dask, dask_cudf\n",
    "from dask.distributed import Client, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names that need to be re-mapped\n",
    "remap = {}\n",
    "remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "#create a list of columns & dtypes the df must have\n",
    "must_haves = {\n",
    " 'pickup_datetime': 'datetime64[ms]',\n",
    " 'dropoff_datetime': 'datetime64[ms]',\n",
    " 'passenger_count': 'int32',\n",
    " 'trip_distance': 'float32',\n",
    " 'pickup_longitude': 'float32',\n",
    " 'pickup_latitude': 'float32',\n",
    " 'rate_code': 'int32',\n",
    " 'dropoff_longitude': 'float32',\n",
    " 'dropoff_latitude': 'float32',\n",
    " 'fare_amount': 'float32'\n",
    "}\n",
    "\n",
    "query_frags = [\n",
    "    'fare_amount > 0 and fare_amount < 500',\n",
    "    'passenger_count > 0 and passenger_count < 6',\n",
    "    'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "    'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "    'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "    'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df_part, remap, must_haves):\n",
    "    tmp = {col:col.strip().lower() for col in list(df_part.columns)}\n",
    "    df_part = df_part.rename(columns=tmp)\n",
    "    # rename using the supplied mapping\n",
    "    df_part = df_part.rename(columns=remap)\n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part = df_part.drop(col, axis=1)\n",
    "            continue\n",
    "\n",
    "        # fixes datetime error found by Ty Mckercher and fixed by Paul Mahler\n",
    "        if df_part[col].dtype == 'object' and col in ['pickup_datetime', 'dropoff_datetime']:\n",
    "            df_part[col] = df_part[col].astype('datetime64[ms]')\n",
    "            continue\n",
    "\n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].str.fillna('-1')\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('float32')\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "    return df_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "\n",
    "def haversine_distance_kernel(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude, h_distance):\n",
    "    for i, (x_1, y_1, x_2, y_2) in enumerate(zip(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude)):\n",
    "        x_1 = pi/180 * x_1\n",
    "        y_1 = pi/180 * y_1\n",
    "        x_2 = pi/180 * x_2\n",
    "        y_2 = pi/180 * y_2\n",
    "        \n",
    "        dlon = y_2 - y_1\n",
    "        dlat = x_2 - x_1\n",
    "        a = sin(dlat/2)**2 + cos(x_1) * cos(x_2) * sin(dlon/2)**2\n",
    "        \n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        r = 6371 # Radius of earth in kilometers\n",
    "        \n",
    "        h_distance[i] = c * r\n",
    "\n",
    "def day_of_the_week_kernel(day, month, year, day_of_week):\n",
    "    for i, (d_1, m_1, y_1) in enumerate(zip(day, month, year)):\n",
    "        if month[i] <3:\n",
    "            shift = month[i]\n",
    "        else:\n",
    "            shift = 0\n",
    "        Y = year[i] - (month[i] < 3)\n",
    "        y = Y - 2000\n",
    "        c = 20\n",
    "        d = day[i]\n",
    "        m = month[i] + shift + 1\n",
    "        day_of_week[i] = (d + math.floor(m*2.6) + y + (y//4) + (c//4) -2*c)%7\n",
    "        \n",
    "def add_features(df):\n",
    "    df['hour'] = df['pickup_datetime'].dt.hour\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "    df['month'] = df['pickup_datetime'].dt.month\n",
    "    df['day'] = df['pickup_datetime'].dt.day\n",
    "    df['diff'] = df['dropoff_datetime'].astype('int32') - df['pickup_datetime'].astype('int32')\n",
    "    \n",
    "    df['pickup_latitude_r'] = df['pickup_latitude']//.01*.01\n",
    "    df['pickup_longitude_r'] = df['pickup_longitude']//.01*.01\n",
    "    df['dropoff_latitude_r'] = df['dropoff_latitude']//.01*.01\n",
    "    df['dropoff_longitude_r'] = df['dropoff_longitude']//.01*.01\n",
    "    \n",
    "    df = df.drop('pickup_datetime', axis=1)\n",
    "    df = df.drop('dropoff_datetime', axis =1)\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(haversine_distance_kernel,\n",
    "                   incols=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'],\n",
    "                   outcols=dict(h_distance=np.float32),\n",
    "                   kwargs=dict())\n",
    "    \n",
    "    \n",
    "    df = df.apply_rows(day_of_the_week_kernel,\n",
    "                      incols=['day', 'month', 'year'],\n",
    "                      outcols=dict(day_of_week=np.float32),\n",
    "                      kwargs=dict())\n",
    "    \n",
    "    \n",
    "    df['is_weekend'] = (df['day_of_week']<2)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"\"\n",
    "resource_group = \"\"\n",
    "vnet = \"\"\n",
    "security_group = \"\"\n",
    "\n",
    "vm_size = \"Standard_NC12s_v3\"\n",
    "docker_image = \"rapidsai/rapidsai:cuda10.2-runtime-ubuntu18.04-py3.8\"\n",
    "worker_class = \"dask_cuda.CUDAWorker\"\n",
    " \n",
    "n_workers = 1\n",
    "env_vars = {\"EXTRA_PIP_PACKAGES\": \"gcsfs\"} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating scheduler instance\n",
      "Assigned public IP\n",
      "Network interface ready\n",
      "Creating VM\n",
      "Created VM dask-ec6ab826-scheduler\n",
      "Waiting for scheduler to run\n",
      "Scheduler is running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/contextlib.py:120: UserWarning: Creating your cluster is taking a surprisingly long time. This is likely due to pending resources. Hang tight! \n",
      "  next(self.gen)\n"
     ]
    }
   ],
   "source": [
    "from distributed import Client\n",
    "from dask_cloudprovider.azure import AzureVMCluster\n",
    "\n",
    "cluster = AzureVMCluster(\n",
    "    location=location,\n",
    "    resource_group=resource_group,\n",
    "    vnet=vnet,\n",
    "    security_group=security_group,\n",
    "    vm_size=vm_size,\n",
    "    docker_image=docker_image,\n",
    "    worker_class=worker_class,\n",
    "    env_vars=env_vars,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_rf_params = {\n",
    "    'n_estimators': 100,\n",
    "    'max_depth': 16,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/distributed/client.py:1129: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "+---------+--------+-----------+---------+\n",
      "| Package | client | scheduler | workers |\n",
      "+---------+--------+-----------+---------+\n",
      "| blosc   | 1.10.1 | None      | None    |\n",
      "| lz4     | 3.1.1  | None      | None    |\n",
      "+---------+--------+-----------+---------+\n",
      "  warnings.warn(version_module.VersionMismatchWarning(msg[0][\"warning\"]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  3.0230505\n"
     ]
    }
   ],
   "source": [
    "with Client(cluster) as client:\n",
    "    import dask_cudf\n",
    "    cluster.scale(2)\n",
    "    client.wait_for_workers(2)\n",
    "    from cuml.dask.ensemble import RandomForestRegressor\n",
    "\n",
    "    base_path = 'gcs://anaconda-public-data/nyc-taxi/csv/'\n",
    "    df_2014 = dask_cudf.read_csv(base_path+'2014/yellow_tripdata_2014*.csv', n_rows=1000)\n",
    "\n",
    "    df_2014 = clean(df_2014, remap, must_haves)\n",
    "\n",
    "    df_2014 = df_2014.query(' and '.join(query_frags))\n",
    "\n",
    "    taxi_df = df_2014.map_partitions(add_features)\n",
    "\n",
    "    taxi_df = taxi_df.dropna()\n",
    "    taxi_df = taxi_df.astype(\"float32\")\n",
    "    X, y = taxi_df.drop([\"fare_amount\"], axis=1), taxi_df[\"fare_amount\"].astype('float32')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
    "\n",
    "    workers = client.has_what().keys()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = dask_utils.persist_across_workers(client,\n",
    "                                                           [X_train, X_test, y_train, y_test],\n",
    "                                                           workers=workers)\n",
    "    cu_dask_rf = RandomForestRegressor(**cu_rf_params, ignore_empty_partitions=True)\n",
    "    cu_dask_rf = cu_dask_rf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = cu_dask_rf.predict(X_test)\n",
    "    \n",
    "    _y_pred, _y_test = y_pred.compute().to_array(), y_test.compute().to_array()\n",
    "    \n",
    "    score = mean_squared_error(_y_pred, _y_test)\n",
    "    print(\"RMSE: \", np.sqrt(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
