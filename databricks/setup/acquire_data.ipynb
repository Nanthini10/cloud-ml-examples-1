{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copy this code to a databricks notebook to download the ORC\n",
    "file of Airlines data and preprocess it for training.\n",
    "\"\"\"\n",
    "import cudf\n",
    "import numpy as np\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import gzip\n",
    "\n",
    "# ACTION REQUIRED - Set the DBFS path \n",
    "data_dir = \"/_dbfs_path/\" # DBFS path to the encompassing folder\n",
    "\n",
    "# We will use airline_small.parquet as default - for benchmarks we used a larger\n",
    "# dataset present at https://rapidsai-cloud-ml-sample-data.s3-us-west-2.amazonaws.com/airline_20000000.parquet\n",
    "# in order to used the larger data - change filename to airline_20000000.parquet\n",
    "\n",
    "\n",
    "file_name = 'airline_small.parquet' # NOTE: Change to airline_20000000.parquet to use a larger dataset\n",
    "parquet_name = os.path.join(data_dir, file_name)\n",
    "\n",
    "def prepare_dataset():\n",
    "\n",
    "    input_cols = [\"Year\", \"Month\", \"DayofMonth\", \"DayofWeek\", \"CRSDepTime\", \"CRSArrTime\",\n",
    "                  \"UniqueCarrier\", \"FlightNum\", \"ActualElapsedTime\", \"Origin\", \"Dest\",\n",
    "                  \"Distance\", \"Diverted\"]\n",
    "\n",
    "    # Download URL \n",
    "    url = \"https://rapidsai-cloud-ml-sample-data.s3-us-west-2.amazonaws.com/\" + file_name\n",
    "\n",
    "    \n",
    "    if os.path.isfile(parquet_name):\n",
    "        print(f\" > File already exists. Ready to load at {parquet_name}\")\n",
    "    else:\n",
    "        # Ensure folder exists\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        def data_progress_hook(block_number, read_size, total_filesize):\n",
    "            if (block_number % 1000) == 0:\n",
    "                print(\n",
    "                    f\" > percent complete: { 100 * ( block_number * read_size ) / total_filesize:.2f}\\r\",\n",
    "                    end=\"\",\n",
    "                )\n",
    "            return\n",
    "        urlretrieve(\n",
    "            url= url,\n",
    "            filename=parquet_name,\n",
    "            reporthook=data_progress_hook,\n",
    "        )\n",
    "        \n",
    "        print(f\" > Download complete {url}\")\n",
    "        \n",
    "    dataset = cudf.read_parquet(parquet_name)\n",
    "\n",
    "    # encode categoricals as numeric\n",
    "    for col in dataset.select_dtypes([\"object\"]).columns:\n",
    "        dataset[col] = dataset[col].astype(\"category\").cat.codes.astype(np.int32)\n",
    "\n",
    "    # cast all columns to int32\n",
    "    for col in dataset.columns:\n",
    "        dataset[col] = dataset[col].astype(np.float32)  # needed for random forest\n",
    "\n",
    "    # put target/label column first [ classic XGBoost standard ]\n",
    "    output_cols = [\"ArrDelayBinary\"] + input_cols\n",
    "\n",
    "    dataset = dataset.reindex(columns=output_cols)\n",
    "    dataset.to_parquet(parquet_name)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "name": "acquire_data",
  "notebookId": 4348299065994525
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
