{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Optuna + RAPIDS\n",
    "\n",
    "Hyperparameter optimization is the task of picking values for the hyperparamters of the model to yield optimal results. This can help boost the model accuracy greatly, but can be resource-intensive and is often not widely used for practical purposes. Let's take a look at how we can use Optuna and RAPIDS to make HPO less time-consuming.\n",
    "\n",
    "\n",
    "## Optuna\n",
    "[Optuna](https://optuna.readthedocs.io/en/stable/) is a lightweight framework for automatic hyperparameter optimization. It provides a define-by-run API, which makes it easy to adapt to any already existing code that we have and enables high modularity and the flexibility to construct hyperparameter spaces dynamically. By simply wrapping the objective function with Optuna can help perform a parallel-distributed HPO search over a search space. As we'll see in this notebook.\n",
    "\n",
    "\n",
    "## RAPIDS\n",
    "RAPIDS framework provides a library suite that can execute end-to-end data science pipelines entirely on GPUs. One of the libraries in this framework is cuML, which contains various Machine Learning algorithms that take advantage of GPU to run. You can learn more about RAPIDS [here](https://rapids.ai/about.html).\n",
    "\n",
    "\n",
    "In this notebook, we'll use Airlines dataset (20M rows) to predict if a flight will be delayed or not. We'll explore how to use Optuna with RAPIDS and the speedups that we can achieve with the integration of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to install optuna and mlflow\n",
    "# !pip install optuna\n",
    "# !pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/treelite/gallery/__init__.py:7: FutureWarning: treelite.gallery.sklearn has been moved to treelite.sklearn. treelite.gallery.sklearn will be removed in version 1.1.\n",
      "  FutureWarning)\n",
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/treelite/gallery/sklearn/__init__.py:9: FutureWarning: treelite.gallery.sklearn has been moved to treelite.sklearn. treelite.gallery.sklearn will be removed in version 1.1.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import cudf\n",
    "import cuml\n",
    "import dask_cudf\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "from cuml.metrics import accuracy_score\n",
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "from dask.distributed import Client, wait, performance_report\n",
    "from joblib import parallel_backend, Parallel, delayed\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from dask_cuda import LocalCUDACluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for timing blocks of code.\n",
    "@contextmanager\n",
    "def timed(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    t1 = time.time()\n",
    "    print(\"..%-24s:  %8.4f\" % (name, t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up CUDA Cluster\n",
    "\n",
    "We start a local cluster and keep it ready for running distributed tasks with dask.\n",
    "\n",
    "[LocalCUDACluster](https://github.com/rapidsai/dask-cuda) launches one Dask worker for each GPU in the current systems. It's developed as a part of the RAPIDS project. Learn More:\n",
    "- [Setting up Dask](https://docs.dask.org/en/latest/setup.html)\n",
    "- [Dask Client](https://distributed.dask.org/en/latest/client.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://172.17.0.2:42787</li>\n",
       "  <li><b>Dashboard: </b><a href='http://172.17.0.2:8002/status' target='_blank'>http://172.17.0.2:8002/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>49.16 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://172.17.0.2:42787' processes=2 threads=2, memory=49.16 GB>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will use all GPUs on the local host by default\n",
    "cluster = LocalCUDACluster(threads_per_worker=1, ip=\"\", dashboard_address=\"8002\")\n",
    "c = Client(cluster)\n",
    "\n",
    "# Query the client for all connected workers\n",
    "workers = c.has_what().keys()\n",
    "n_workers = len(workers)\n",
    "n_streams = 8 # Performance optimization\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "### Data Acquisition\n",
    "\n",
    "We'll use the cell below to download the data. The `file_name` specifies which of the two available files - airline_small.parquet (smaller file) and airline_20000000.parquet we want to use. By default, we use the smaller file, but the benchmarks were run with the larger file. You are free to change it for experimentation.\n",
    "\n",
    "Run the cell below just once in the local machine to acquire data. Comment out for future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read above instructions - RUN ONLY ONCE\n",
    "\n",
    "# from urllib.request import urlretrieve\n",
    "# import os\n",
    "\n",
    "# file_name = 'airline_small.parquet' # NOTE: Change to airline_20000000.parquet to use a larger dataset\n",
    "\n",
    "# data_dir = \"/local/path/to/file/\" # NOTE: Change to a local path where you want to save the file\n",
    "# INPUT_FILE = os.path.join(data_dir, file_name)\n",
    "\n",
    "# if os.path.isfile(INPUT_FILE):\n",
    "#         print(f\" > File already exists. Ready to load at {INPUT_FILE}\")\n",
    "# else:\n",
    "#     # Ensure folder exists\n",
    "#     os.makedirs(data_dir, exist_ok=True)\n",
    "        \n",
    "# url = \"https://rapidsai-cloud-ml-sample-data.s3-us-west-2.amazonaws.com/\" + file_name\n",
    "\n",
    "# urlretrieve(url= url,filename=INPUT_FILE)\n",
    "\n",
    "# print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the `N_TRIALS` for the number of runs of HPO trials. \n",
    "\n",
    "We will now, load the data from `INPUT_FILE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 20\n",
    "\n",
    "INPUT_FILE = \"/home/hyperopt/hyperopt/data/air_par.parquet\"\n",
    "df = cudf.read_parquet(INPUT_FILE)\n",
    "X, y = df.drop([\"ArrDelayBinary\"], axis=1), df[\"ArrDelayBinary\"].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation\n",
    "\n",
    "Here, we define `train_and_eval` function which simply fits a RandomForestClassifier (with`max_depth` and `n_estimators`) on the passed `X_param`, `y_param`. This function should look very similar for any ML workflow. We'll use this function within the Optuna `objective` function to show how easily we can fit an existing workflow into the Optuna work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(X_param, y_param, max_depth=16, n_estimators=100):\n",
    "    \"\"\"\n",
    "        Splits the given data into train and test split to train and evaluate the model\n",
    "        for the params parameters.\n",
    "        \n",
    "        Params\n",
    "        ______\n",
    "        \n",
    "        X_param:  DataFrame. \n",
    "                  The data to use for training and testing. \n",
    "        y_param:  Series. \n",
    "                  The label for training\n",
    "        max_depth, n_estimators: The values to use for max_depth and n_estimators for RFC.\n",
    "                                 Defaults to 16 and 100 (the defaults for the classifiers used)\n",
    "                   \n",
    "        Returns\n",
    "        score: Accuracy score of the fitted model\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_param, y_param, random_state=77)\n",
    "    classifier = cuml.ensemble.RandomForestClassifier(max_depth=max_depth,\n",
    "                     n_estimators=n_estimators)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_valid)\n",
    "    score = accuracy_score(y_valid, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a baseline number, let's see what the default performance of RFC is. Note the defauly values for `max_depth` = 16 and `n_estimators` = 100; we pass these to the `train_and_eval` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with default parameters :  0.837492823600769\n"
     ]
    }
   ],
   "source": [
    "print(\"Score with default parameters : \",train_and_eval(X, y, max_depth=16, n_estimators=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "\n",
    "The objective function will be the one we optimize in [Optuna Study](https://optuna.readthedocs.io/en/stable/reference/study.html). Objective funciton tries out different values for the parameters that we are tuning and saving the results in `study.trials_dataframes()`. \n",
    "\n",
    "Let's define the objective function for this HPO task by making use of the `train_and_eval()`. You can see that we simply choose a value for the parameters and call the `train_and_eval` method, making Optuna very easy to use in an existing workflow.\n",
    "\n",
    "The objective remains constant over different [samplers](https://optuna.readthedocs.io/en/stable/reference/samplers.html), which are built-in options in Optuna to enable the selection of different sampling algorithms that optuna provides. Some of the available ones include - GridSampler, RandomSampler, TPESampler, etc. We'll use TPESampler for this demo, but feel free to try different samplers to notice the chnages in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_param, y_param):\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 10, 15)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 200, 700)\n",
    "    score = train_and_eval(X_param, y_param, max_depth=max_depth,\n",
    "                           n_estimators=n_estimators)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO Trials and Study\n",
    "\n",
    "Optuna uses [study](https://optuna.readthedocs.io/en/stable/reference/study.html) and [trials](https://optuna.readthedocs.io/en/stable/reference/trial.html) to keep track of the HPO experiments. Put simply, a trial is a single call of the objective function while a set of trials make up a study. We will pick the optimal performing trial from a study to get the best parameters that were used in that run.\n",
    "\n",
    "We'll make use of a helper function `run_study` to help us run one multi-GPU study with a dask backend using [Joblib](https://joblib.readthedocs.io/en/latest/). We make use of `parallel_backend` from Joblib to allow the optuna studies to run in parallel on the GPUs as we make use of the dask backend. As you can observe, it accepts the client that we set up earlier and the data X and y are scattered across them - this reduces the time it takes for data transfer among the GPUs.\n",
    "\n",
    "Optuna also requires the used of a storage to run distributed optimization runs. Learn more about what storages can be used [here](https://optuna.readthedocs.io/en/stable/tutorial/distributed.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_study(sampler=optuna.samplers.TPESampler(),\n",
    "              study_name=\"Optuna-MultiGPU\",\n",
    "              callbacks=None):\n",
    "    \n",
    "    with timed(study_name):\n",
    "        study = optuna.create_study(sampler=sampler,\n",
    "                                    study_name=study_name,\n",
    "                                    storage=\"sqlite:///_\"+study_name+\".db\",\n",
    "                                    direction=\"maximize\",\n",
    "                                    load_if_exists=True)\n",
    "        \n",
    "        with parallel_backend(\"dask\", n_jobs=n_workers, client=c, scatter=[X,y]):\n",
    "            study.optimize(lambda trial: objective(trial, X, y),\n",
    "                           n_trials=N_TRIALS,\n",
    "                           n_jobs=n_workers,\n",
    "                           callbacks=callbacks)\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Value: \", trial.value)\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-07 00:00:34,923] A new study created with name: optuna-joblib-dask-backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..optuna-joblib-dask-backend:  526.3728\n",
      "Number of finished trials:  20\n",
      "Best trial:\n",
      "  Value:  0.8430033922195435\n",
      "  Params: \n",
      "    max_depth: 15\n",
      "    n_estimators: 502\n"
     ]
    }
   ],
   "source": [
    "name = \"optuna-joblib-dask-backend\"\n",
    "with performance_report(filename=name+\"-dask_report.html\"):\n",
    "    study_tpe = run_study(optuna.samplers.TPESampler(),\n",
    "                          study_name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, how long would it take without dask or multiprocessing with joblib?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-07 00:24:35,853] A new study created with name: optuna-simple\n",
      "[I 2020-07-07 00:25:12,172] Finished trial#0 with value: 0.8307780027389526 with parameters: {'max_depth': 13, 'n_estimators': 514}. Best is trial#0 with value: 0.8307780027389526.\n",
      "[I 2020-07-07 00:25:36,235] Finished trial#1 with value: 0.8307763934135437 with parameters: {'max_depth': 13, 'n_estimators': 343}. Best is trial#0 with value: 0.8307780027389526.\n",
      "[I 2020-07-07 00:25:53,840] Finished trial#2 with value: 0.8308228254318237 with parameters: {'max_depth': 14, 'n_estimators': 221}. Best is trial#2 with value: 0.8308228254318237.\n",
      "[I 2020-07-07 00:26:23,624] Finished trial#3 with value: 0.8342193961143494 with parameters: {'max_depth': 10, 'n_estimators': 640}. Best is trial#3 with value: 0.8342193961143494.\n",
      "[I 2020-07-07 00:26:41,571] Finished trial#4 with value: 0.8341361880302429 with parameters: {'max_depth': 10, 'n_estimators': 379}. Best is trial#3 with value: 0.8342193961143494.\n",
      "[I 2020-07-07 00:27:01,922] Finished trial#5 with value: 0.8308327794075012 with parameters: {'max_depth': 12, 'n_estimators': 327}. Best is trial#3 with value: 0.8342193961143494.\n",
      "[I 2020-07-07 00:27:22,114] Finished trial#6 with value: 0.8308448195457458 with parameters: {'max_depth': 14, 'n_estimators': 252}. Best is trial#3 with value: 0.8342193961143494.\n",
      "[I 2020-07-07 00:27:56,312] Finished trial#7 with value: 0.8427703976631165 with parameters: {'max_depth': 15, 'n_estimators': 268}. Best is trial#7 with value: 0.8427703976631165.\n",
      "[I 2020-07-07 00:28:32,705] Finished trial#8 with value: 0.8308395743370056 with parameters: {'max_depth': 12, 'n_estimators': 584}. Best is trial#7 with value: 0.8427703976631165.\n",
      "[I 2020-07-07 00:28:53,827] Finished trial#9 with value: 0.8307684063911438 with parameters: {'max_depth': 13, 'n_estimators': 301}. Best is trial#7 with value: 0.8427703976631165.\n",
      "[I 2020-07-07 00:29:51,463] Finished trial#10 with value: 0.8428760170936584 with parameters: {'max_depth': 15, 'n_estimators': 451}. Best is trial#10 with value: 0.8428760170936584.\n",
      "[I 2020-07-07 00:30:48,392] Finished trial#11 with value: 0.842991828918457 with parameters: {'max_depth': 15, 'n_estimators': 453}. Best is trial#11 with value: 0.842991828918457.\n",
      "[I 2020-07-07 00:31:45,381] Finished trial#12 with value: 0.8428137898445129 with parameters: {'max_depth': 15, 'n_estimators': 452}. Best is trial#11 with value: 0.842991828918457.\n",
      "[I 2020-07-07 00:32:40,915] Finished trial#13 with value: 0.8428832292556763 with parameters: {'max_depth': 15, 'n_estimators': 441}. Best is trial#11 with value: 0.842991828918457.\n",
      "[I 2020-07-07 00:33:23,365] Finished trial#14 with value: 0.8307920098304749 with parameters: {'max_depth': 14, 'n_estimators': 526}. Best is trial#11 with value: 0.842991828918457.\n",
      "[I 2020-07-07 00:34:14,538] Finished trial#15 with value: 0.8429955840110779 with parameters: {'max_depth': 15, 'n_estimators': 407}. Best is trial#15 with value: 0.8429955840110779.\n",
      "[I 2020-07-07 00:34:35,717] Finished trial#16 with value: 0.8310868144035339 with parameters: {'max_depth': 11, 'n_estimators': 393}. Best is trial#15 with value: 0.8429955840110779.\n",
      "[I 2020-07-07 00:35:19,450] Finished trial#17 with value: 0.8307818174362183 with parameters: {'max_depth': 14, 'n_estimators': 539}. Best is trial#15 with value: 0.8429955840110779.\n",
      "[I 2020-07-07 00:36:10,239] Finished trial#18 with value: 0.8428857922554016 with parameters: {'max_depth': 15, 'n_estimators': 403}. Best is trial#15 with value: 0.8429955840110779.\n",
      "[I 2020-07-07 00:37:05,997] Finished trial#19 with value: 0.8308162093162537 with parameters: {'max_depth': 14, 'n_estimators': 685}. Best is trial#15 with value: 0.8429955840110779.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..no-dask-no-joblib       :  750.3215\n"
     ]
    }
   ],
   "source": [
    "name = \"optuna-simple\"\n",
    "with timed(\"no-dask-no-joblib\"):\n",
    "    study = optuna.create_study(sampler=optuna.samplers.TPESampler(),\n",
    "                                study_name=name,\n",
    "                                storage=\"sqlite:///_\"+name+\".db\",\n",
    "                                direction=\"maximize\",\n",
    "                                load_if_exists=True)\n",
    "    study.optimize(lambda trial: objective(trial, X, y), n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectely it takes a lot longer to run a sequential Optuna call\n",
    "\n",
    "\n",
    "# Sequential calls without Optuna\n",
    "\n",
    "For a comparison let's try sequential calls without Optuna and it's parallel-processing support. We'll pick the same parameters as Optuna for a fair comparison - these parameters were selected by the sampling algorithm used by Optuna and is available in the `study.trials_dataframe()` for us to pick out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = study_tpe.trials_dataframe()\n",
    "params_max_depth, params_n_estimators = df['params_max_depth'], df['params_n_estimators']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential call function \n",
    "\n",
    "For a cleaner look, let's use a function to perform sequential calls. The function basically sets the parameters to what was passed and trains and evaluates the model and returns the details of the run which can later be used to find the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_call(X, y, max_depth, n_estimators):\n",
    "    \n",
    "    score = train_and_eval(X, y, max_depth=max_depth, n_estimators = n_estimators)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..sequential-calls        :  857.3299\n",
      "0.843004584312439 15 502\n"
     ]
    }
   ],
   "source": [
    "name = \"sequential-calls\"\n",
    "max_acc = -1\n",
    "with timed(name):\n",
    "    for i in range(N_TRIALS):\n",
    "        acc = seq_call(X, y, max_depth=params_max_depth[i],\n",
    "                     n_estimators=params_n_estimators[i])\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "            best_max_depth = params_max_depth[i]\n",
    "            best_n_est = params_n_estimators[i]\n",
    "\n",
    "print(max_acc, best_max_depth, best_n_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running it without any framework expectedly takes the longest to run. \n",
    "\n",
    "## Summarizing the timing results\n",
    "\n",
    "| Study name | Runtime |   \n",
    "|---|---|\n",
    "| Optuna-Multi-GPU-TPE | 526.3728 |\n",
    "| No-dask-No-Joblib | 750.3215 |\n",
    "| No-Optuna-No-dask-Seq-Call | 857.3299 |\n",
    "\n",
    "We see the Optuna with parallel runs of Single-GPU estimators yields much better results than the alternatives. This will help run more iterations of hyperparameter search and yield quicker results. This can serve as a good starting point to try out Optuna + RAPIDS to see faster HPO runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
