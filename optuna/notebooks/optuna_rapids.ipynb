{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Optuna + RAPIDS\n",
    "\n",
    "Hyperparameter optimization is the task of picking values for the hyperparamters of the model to yield optimal results. This can help boost the model accuracy greatly, but can be resource-intensive and is often not widely used for practical purposes. Let's take a look at how we can use Optuna and RAPIDS to make HPO less time-consuming.\n",
    "\n",
    "\n",
    "## Optuna\n",
    "[Optuna](https://optuna.readthedocs.io/en/stable/) is a lightweight framework for automatic hyperparameter optimization. It provides a define-by-run API, which makes it easy to adapt to any already existing code that we have and enables high modularity and the flexibility to construct hyperparameter spaces dynamically. By simply wrapping the objective function with Optuna can help perform a parallel-distributed HPO search over a search space. As we'll see in this notebook.\n",
    "\n",
    "\n",
    "## RAPIDS\n",
    "RAPIDS framework provides a library suite that can execute end-to-end data science pipelines entirely on GPUs. One of the libraries in this framework is cuML, which contains various Machine Learning algorithms that take advantage of GPU to run. You can learn more about RAPIDS [here](https://rapids.ai/about.html).\n",
    "\n",
    "\n",
    "In this notebook, we'll use Airlines dataset (20M rows) to predict if a flight will be delayed or not. We'll explore how to use Optuna with RAPIDS in combination with Dask to run multi-GPU HPO runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to install optuna and mlflow\n",
    "# !pip install optuna\n",
    "# !pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import cudf\n",
    "import cuml\n",
    "import dask_cudf\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "from cuml.dask.common import utils as dask_utils\n",
    "from cuml.metrics import accuracy_score\n",
    "from cuml.preprocessing.model_selection import train_test_split\n",
    "from dask.distributed import Client, wait, performance_report\n",
    "from joblib import parallel_backend, Parallel, delayed\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from dask_cuda import LocalCUDACluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for timing blocks of code.\n",
    "@contextmanager\n",
    "def timed(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    t1 = time.time()\n",
    "    print(\"..%-24s:  %8.4f\" % (name, t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up CUDA Cluster\n",
    "\n",
    "We start a local cluster and keep it ready for running distributed tasks with dask.\n",
    "\n",
    "[LocalCUDACluster](https://github.com/rapidsai/dask-cuda) launches one Dask worker for each GPU in the current systems. It's developed as a part of the RAPIDS project. Learn More:\n",
    "- [Setting up Dask](https://docs.dask.org/en/latest/setup.html)\n",
    "- [Dask Client](https://distributed.dask.org/en/latest/client.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://172.17.0.4:38793</li>\n",
       "  <li><b>Dashboard: </b><a href='http://172.17.0.4:8002/status' target='_blank'>http://172.17.0.4:8002/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>49.16 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://172.17.0.4:38793' processes=2 threads=2, memory=49.16 GB>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will use all GPUs on the local host by default\n",
    "cluster = LocalCUDACluster(threads_per_worker=1, ip=\"\", dashboard_address=\"8002\")\n",
    "c = Client(cluster)\n",
    "\n",
    "# Query the client for all connected workers\n",
    "workers = c.has_what().keys()\n",
    "n_workers = len(workers)\n",
    "n_streams = 8 # Performance optimization\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "### Data Acquisition\n",
    "\n",
    "We'll use the cell below to download the data. The `file_name` specifies which of the two available files - airline_small.parquet (smaller file) and airline_20000000.parquet we want to use. By default, we use the smaller file, but the benchmarks were run with the larger file. You are free to change it for experimentation.\n",
    "\n",
    "Set the `download_data=True` and `data_dir` to a local path in your system to download the data into the path of `data_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data = False\n",
    "\n",
    "file_name = 'airline_small.parquet' # NOTE: Change to airline_20000000.parquet to use a larger dataset\n",
    "\n",
    "data_dir = \"/home/data/airline_data/\" # NOTE: Change to a local path where you want to save the file\n",
    "INPUT_FILE = os.path.join(data_dir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if download_data:\n",
    "    from urllib.request import urlretrieve\n",
    "\n",
    "    if os.path.isfile(INPUT_FILE):\n",
    "            print(f\" > File already exists. Ready to load at {INPUT_FILE}\")\n",
    "    else:\n",
    "        # Ensure folder exists\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    url = \"https://rapidsai-cloud-ml-sample-data.s3-us-west-2.amazonaws.com/\" + file_name\n",
    "\n",
    "    urlretrieve(url= url,filename=INPUT_FILE)\n",
    "\n",
    "    print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the `N_TRIALS` for the number of runs of HPO trials. \n",
    "\n",
    "We will now, load the data from `INPUT_FILE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 20\n",
    "\n",
    "df = cudf.read_parquet(INPUT_FILE)\n",
    "X, y = df.drop([\"ArrDelayBinary\"], axis=1), df[\"ArrDelayBinary\"].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation\n",
    "\n",
    "Here, we define `train_and_eval` function which simply fits a RandomForestClassifier (with`max_depth` and `n_estimators`) on the passed `X_param`, `y_param`. This function should look very similar for any ML workflow. We'll use this function within the Optuna `objective` function to show how easily we can fit an existing workflow into the Optuna work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(X_param, y_param, max_depth=16, n_estimators=100):\n",
    "    \"\"\"\n",
    "        Splits the given data into train and test split to train and evaluate the model\n",
    "        for the params parameters.\n",
    "        \n",
    "        Params\n",
    "        ______\n",
    "        \n",
    "        X_param:  DataFrame. \n",
    "                  The data to use for training and testing. \n",
    "        y_param:  Series. \n",
    "                  The label for training\n",
    "        max_depth, n_estimators: The values to use for max_depth and n_estimators for RFC.\n",
    "                                 Defaults to 16 and 100 (the defaults for the classifiers used)\n",
    "                   \n",
    "        Returns\n",
    "        score: Accuracy score of the fitted model\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_param, y_param, random_state=77)\n",
    "    classifier = cuml.ensemble.RandomForestClassifier(max_depth=max_depth,\n",
    "                     n_estimators=n_estimators)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_valid)\n",
    "    score = accuracy_score(y_valid, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a baseline number, let's see what the default performance of RFC is. Note the defauly values for `max_depth` = 16 and `n_estimators` = 100; we pass these to the `train_and_eval` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with default parameters :  0.8378592133522034\n"
     ]
    }
   ],
   "source": [
    "print(\"Score with default parameters : \",train_and_eval(X, y, max_depth=16, n_estimators=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "\n",
    "The objective function will be the one we optimize in [Optuna Study](https://optuna.readthedocs.io/en/stable/reference/study.html). Objective funciton tries out different values for the parameters that we are tuning and saving the results in `study.trials_dataframes()`. \n",
    "\n",
    "Let's define the objective function for this HPO task by making use of the `train_and_eval()`. You can see that we simply choose a value for the parameters and call the `train_and_eval` method, making Optuna very easy to use in an existing workflow.\n",
    "\n",
    "The objective remains constant over different [samplers](https://optuna.readthedocs.io/en/stable/reference/samplers.html), which are built-in options in Optuna to enable the selection of different sampling algorithms that optuna provides. Some of the available ones include - GridSampler, RandomSampler, TPESampler, etc. We'll use TPESampler for this demo, but feel free to try different samplers to notice the chnages in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, X_param, y_param):\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 10, 15)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 200, 700)\n",
    "    score = train_and_eval(X_param, y_param, max_depth=max_depth,\n",
    "                           n_estimators=n_estimators)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO Trials and Study\n",
    "\n",
    "Optuna uses [study](https://optuna.readthedocs.io/en/stable/reference/study.html) and [trials](https://optuna.readthedocs.io/en/stable/reference/trial.html) to keep track of the HPO experiments. Put simply, a trial is a single call of the objective function while a set of trials make up a study. We will pick the optimal performing trial from a study to get the best parameters that were used in that run.\n",
    "\n",
    "We'll make use of a helper function `run_study` to help us run one multi-GPU study with a dask backend using [Joblib](https://joblib.readthedocs.io/en/latest/). We make use of `parallel_backend` from Joblib to allow the optuna studies to run in parallel on the GPUs as we make use of the dask backend. As you can observe, it accepts the client that we set up earlier and the data X and y are scattered across them - this reduces the time it takes for data transfer among the GPUs.\n",
    "\n",
    "Optuna also requires the used of a storage to run distributed optimization runs. Learn more about what storages can be used [here](https://optuna.readthedocs.io/en/stable/tutorial/distributed.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_study(sampler=optuna.samplers.TPESampler(),\n",
    "              study_name=\"Optuna-MultiGPU\",\n",
    "              callbacks=None):\n",
    "    \n",
    "    with timed(study_name):\n",
    "        study = optuna.create_study(sampler=sampler,\n",
    "                                    study_name=study_name,\n",
    "                                    storage=\"sqlite:///_\"+study_name+\".db\",\n",
    "                                    direction=\"maximize\",\n",
    "                                    load_if_exists=True)\n",
    "        \n",
    "        with parallel_backend(\"dask\", n_jobs=n_workers, client=c, scatter=[X,y]):\n",
    "            study.optimize(lambda trial: objective(trial, X, y),\n",
    "                           n_trials=N_TRIALS,\n",
    "                           n_jobs=n_workers,\n",
    "                           callbacks=callbacks)\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"  Value: \", trial.value)\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-08-26 21:18:03,527] Using an existing study with name 'optuna-joblib-dask-backend' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..optuna-joblib-dask-backend:  336.8222\n",
      "Number of finished trials:  100\n",
      "Best trial:\n",
      "  Value:  1.0\n",
      "  Params: \n",
      "    max_depth: 11\n",
      "    n_estimators: 639\n"
     ]
    }
   ],
   "source": [
    "name = \"optuna-joblib-dask-backend\"\n",
    "with performance_report(filename=name+\"-dask_report.html\"):\n",
    "    study_tpe = run_study(optuna.samplers.TPESampler(),\n",
    "                          study_name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
